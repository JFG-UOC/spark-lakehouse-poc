apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-defaults
data:
  spark-defaults.conf: |-
    # Spark master for standalone mode
    spark.master                      spark://spark-master-svc:7077

    # Fixed driver and blockManager ports (for Kubernetes networking)
    spark.driver.bindAddress          0.0.0.0
    spark.driver.port                 39000
    spark.blockManager.port           39001

    # Hive metastore integration
    spark.sql.catalogImplementation   hive
    spark.hadoop.hive.metastore.uris  thrift://hive-metastore-svc:9083

    # Recommended network timeout
    spark.network.timeout             300s

    # Enable event logging (optional)
    spark.eventLog.enabled            true
    spark.eventLog.dir                /tmp/spark-events

    #
    spark.driver.extraClassPath       /opt/spark/ext-jars/*
    spark.executor.extraClassPath     /opt/spark/ext-jars/*
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-hadoop-core-site
data:
  core-site.xml: |-
    <configuration>
      <!-- MinIO endpoint -->
      <property>
        <name>fs.s3a.endpoint</name>
        <value>http://minio-svc:9000</value>
      </property>

      <!-- Path-style access required for MinIO -->
      <property>
        <name>fs.s3a.path.style.access</name>
        <value>true</value>
      </property>

      <!-- S3A client implementation -->
      <property>
        <name>fs.s3a.impl</name>
        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
      </property>

      <!-- SSL disabled because MinIO endpoint is HTTP -->
      <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>false</value>
      </property>

      <!-- Credential provider chain (ENV providers for MinIO) -->
      <property>
        <name>fs.s3a.aws.credentials.provider</name>
        <value>
          software.amazon.awssdk.auth.credentials.EnvironmentVariableCredentialsProvider,
          com.amazonaws.auth.EnvironmentVariableCredentialsProvider
        </value>
      </property>

      <!-- Optional: more stable retry policy -->
      <property>
        <name>fs.s3a.attempts.maximum</name>
        <value>5</value>
      </property>
    </configuration>
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark
      role: master
  template:
    metadata:
      labels:
        app: spark
        role: master
    spec:
      initContainers:
        - name: fetch-jars
          image: curlimages/curl:8.11.1
          command:
            - sh
            - -c
            - |
              set -e
              mkdir -p /ext-jars

              echo "Downloading hadoop-aws 3.4.1..."
              curl -fsSL -o /ext-jars/hadoop-aws-3.4.1.jar \
                https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar

              echo "Downloading AWS SDK bundle..."
              curl -fsSL -o /ext-jars/bundle-2.38.5.jar \
                https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.38.5/bundle-2.38.5.jar

              echo "Downloading Delta Lake for Spark..."
              curl -fsSL -o /ext-jars/delta-spark_2.13-4.0.0.jar \
                https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar
              curl -fsSL -o /ext-jars/delta-storage-4.0.0.jar \
                https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar
          volumeMounts:
            - name: ext-jars
              mountPath: /ext-jars
      containers:
        - name: master
          image: apache/spark:4.0.1
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash","-lc"]
          args:
            - |
              /opt/spark/sbin/start-master.sh \
                --host 0.0.0.0 \
                --port 7077 \
                --webui-port 8080
              tail -f /opt/spark/logs/* &
              wait
          env:
            - name: HOME
              value: "/tmp"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_USER
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_PASSWORD
          ports:
            - containerPort: 6066
            - containerPort: 7077
            - containerPort: 8080
            - containerPort: 39000
            - containerPort: 39001
          volumeMounts:
            - name: spark-defaults
              mountPath: /opt/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: hadoop-core-site
              mountPath: /opt/spark/conf/core-site.xml
              subPath: core-site.xml
            - name: ext-jars
              mountPath: /opt/spark/ext-jars
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "1"
              memory: "1Gi"
      volumes:
        - name: spark-defaults
          configMap:
            name: spark-defaults
        - name: hadoop-core-site
          configMap:
            name: spark-hadoop-core-site
        - name: ext-jars
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: spark-master-svc
spec:
  selector:
    app: spark
    role: master
  ports:
    - name: rest
      port: 6066
      targetPort: 6066
    - name: cluster
      port: 7077
      targetPort: 7077
    - name: ui
      port: 8080
      targetPort: 8080
    - name: driver-rpc
      port: 39000
      targetPort: 39000
    - name: blockmanager
      port: 39001
      targetPort: 39001
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-worker
spec:
  replicas: 2
  selector:
    matchLabels:
      app: spark
      role: worker
  template:
    metadata:
      labels:
        app: spark
        role: worker
    spec:
      initContainers:
        - name: fetch-jars
          image: curlimages/curl:8.11.1
          command:
            - sh
            - -c
            - |
              set -e
              mkdir -p /ext-jars

              echo "Downloading hadoop-aws 3.4.1..."
              curl -fsSL -o /ext-jars/hadoop-aws-3.4.1.jar \
                https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar

              echo "Downloading AWS SDK bundle..."
              curl -fsSL -o /ext-jars/bundle-2.38.5.jar \
                https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.38.5/bundle-2.38.5.jar

              echo "Downloading Delta Lake for Spark..."
              curl -fsSL -o /ext-jars/delta-spark_2.13-4.0.0.jar \
                https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar
              curl -fsSL -o /ext-jars/delta-storage-4.0.0.jar \
                https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar
          volumeMounts:
            - name: ext-jars
              mountPath: /ext-jars
      containers:
        - name: worker
          image: apache/spark:4.0.1
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash","-lc"]
          args:
            - |
              /opt/spark/sbin/start-worker.sh \
                spark://spark-master-svc:7077 \
                --cores 4 \
                --webui-port 8081
              tail -f /opt/spark/logs/* &
              wait
          env:
            - name: HOME
              value: "/tmp"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_USER
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_PASSWORD
          ports:
            - containerPort: 8081
          volumeMounts:
            - name: spark-defaults
              mountPath: /opt/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: hadoop-core-site
              mountPath: /opt/spark/conf/core-site.xml
              subPath: core-site.xml
            - name: ext-jars
              mountPath: /opt/spark/ext-jars
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "4"
              memory: "8Gi"
      volumes:
        - name: spark-defaults
          configMap:
            name: spark-defaults
        - name: hadoop-core-site
          configMap:
            name: spark-hadoop-core-site
        - name: ext-jars
          emptyDir: {}
