apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-defaults
data:
  spark-defaults.conf: |-
    # Spark master for standalone mode
    spark.master                      spark://spark-master-svc:7077

    # Fixed driver and blockManager ports
    spark.driver.bindAddress          0.0.0.0
    spark.driver.port                 39000
    spark.blockManager.port           39001

    # Hive metastore
    spark.sql.catalogImplementation   hive
    spark.hadoop.hive.metastore.uris  thrift://hive-metastore-svc:9083
    spark.sql.hive.metastore.version  3.1.3
    spark.sql.hive.metastore.jars     maven

    spark.network.timeout             300s

    # Event logging
    spark.eventLog.enabled            true
    spark.eventLog.dir                s3a://data/spark-events

    spark.driver.extraClassPath       /opt/spark/ext-jars/*
    spark.executor.extraClassPath     /opt/spark/ext-jars/*
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-hadoop-core-site
data:
  core-site.xml: |-
    <configuration>
      <property>
        <name>fs.s3a.endpoint</name>
        <value>http://minio-svc:9000</value>
      </property>
      <property>
        <name>fs.s3a.path.style.access</name>
        <value>true</value>
      </property>
      <property>
        <name>fs.s3a.impl</name>
        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
      </property>
      <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>fs.s3a.aws.credentials.provider</name>
        <value>
          com.amazonaws.auth.EnvironmentVariableCredentialsProvider,
          com.amazonaws.auth.InstanceProfileCredentialsProvider
        </value>
      </property>
      <property>
        <name>fs.s3a.attempts.maximum</name>
        <value>5</value>
      </property>
    </configuration>

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark
      role: master
  template:
    metadata:
      labels:
        app: spark
        role: master
    spec:

      initContainers:
        - name: fetch-jars
          image: curlimages/curl:8.11.1
          command: ["sh","-c","
            set -e;
            mkdir -p /ext-jars;

            echo 'Downloading hadoop-aws...';
            curl -fsSL -o /ext-jars/hadoop-aws-3.3.4.jar \
              https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar;

            echo 'Downloading AWS SDK...';
            curl -fsSL -o /ext-jars/aws-java-sdk-bundle-1.12.262.jar \
              https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar;

            echo 'Downloading Delta Lake...';
            curl -fsSL -o /ext-jars/delta-spark_2.12-3.3.2.jar \
              https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.2/delta-spark_2.12-3.3.2.jar;
            curl -fsSL -o /ext-jars/delta-storage-3.3.2.jar \
              https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.2/delta-storage-3.3.2.jar;

          "]
          volumeMounts:
            - name: ext-jars
              mountPath: /ext-jars

      containers:
        - name: master
          image: apache/spark:3.5.6
          command: ["/bin/bash","-lc"]
          args:
            - |
              /opt/spark/sbin/start-master.sh --host 0.0.0.0 --port 7077 --webui-port 8080
              tail -f /opt/spark/logs/* & wait

          env:
            - name: HOME
              value: "/tmp"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_USER
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_PASSWORD

          ports:
            - containerPort: 6066
            - containerPort: 7077
            - containerPort: 8080
            - containerPort: 39000
            - containerPort: 39001

          volumeMounts:
            - name: spark-defaults
              mountPath: /opt/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: hadoop-core-site
              mountPath: /opt/spark/conf/core-site.xml
              subPath: core-site.xml
            - name: ext-jars
              mountPath: /opt/spark/ext-jars

      volumes:
        - name: spark-defaults
          configMap:
            name: spark-defaults
        - name: hadoop-core-site
          configMap:
            name: spark-hadoop-core-site
        - name: ext-jars
          emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: spark-master-svc
spec:
  selector:
    app: spark
    role: master
  ports:
    - name: rest
      port: 6066
    - name: cluster
      port: 7077
    - name: ui
      port: 8080
    - name: driver-rpc
      port: 39000
    - name: blockmanager
      port: 39001

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-worker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark
      role: worker
  template:
    metadata:
      labels:
        app: spark
        role: worker
    spec:

      initContainers:
        - name: fetch-jars
          image: curlimages/curl:8.11.1
          command: ["sh","-c","
            set -e;
            mkdir -p /ext-jars;

            echo 'Downloading hadoop-aws...';
            curl -fsSL -o /ext-jars/hadoop-aws-3.3.4.jar \
              https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar;

            echo 'Downloading AWS SDK...';
            curl -fsSL -o /ext-jars/aws-java-sdk-bundle-1.12.262.jar \
              https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar;

            echo 'Downloading Delta Lake...';
            curl -fsSL -o /ext-jars/delta-spark_2.12-3.3.2.jar \
              https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.2/delta-spark_2.12-3.3.2.jar;
            curl -fsSL -o /ext-jars/delta-storage-3.3.2.jar \
              https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.2/delta-storage-3.3.2.jar;

          "]
          volumeMounts:
            - name: ext-jars
              mountPath: /ext-jars

      containers:
        - name: worker
          image: apache/spark:3.5.6
          command: ["/bin/bash","-lc"]
          args:
            - |
              /opt/spark/sbin/start-worker.sh \
                spark://spark-master-svc:7077 \
                --cores 8 \
                --webui-port 8081
              tail -f /opt/spark/logs/* & wait

          env:
            - name: HOME
              value: "/tmp"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_USER
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_PASSWORD

          ports:
            - containerPort: 8081

          volumeMounts:
            - name: spark-defaults
              mountPath: /opt/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: hadoop-core-site
              mountPath: /opt/spark/conf/core-site.xml
              subPath: core-site.xml
            - name: ext-jars
              mountPath: /opt/spark/ext-jars

          resources:
            requests:
              cpu: "1"
              memory: "1Gi"
            limits:
              cpu: "8"
              memory: "16Gi"

      volumes:
        - name: spark-defaults
          configMap:
            name: spark-defaults
        - name: hadoop-core-site
          configMap:
            name: spark-hadoop-core-site
        - name: ext-jars
          emptyDir: {}
