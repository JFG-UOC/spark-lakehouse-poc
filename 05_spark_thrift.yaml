apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-thrift
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark
      role: thrift
  template:
    metadata:
      labels:
        app: spark
        role: thrift
    spec:
      initContainers:
        - name: fetch-jars
          image: curlimages/curl:8.11.1
          command:
            - sh
            - -c
            - |
              set -e
              mkdir -p /ext-jars

              echo "Downloading hadoop-aws 3.4.1..."
              curl -fsSL -o /ext-jars/hadoop-aws-3.4.1.jar \
                https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar

              echo "Downloading AWS SDK bundle..."
              curl -fsSL -o /ext-jars/bundle-2.38.5.jar \
                https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.38.5/bundle-2.38.5.jar

              echo "Downloading Delta Lake for Spark..."
              curl -fsSL -o /ext-jars/delta-spark_2.13-4.0.0.jar \
                https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar
              curl -fsSL -o /ext-jars/delta-storage-4.0.0.jar \
                https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar
          volumeMounts:
            - name: ext-jars
              mountPath: /ext-jars
      containers:
        - name: thrift
          image: apache/spark:4.0.1
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash","-lc"]
          args:
            - |
              mkdir -p /tmp/.ivy2 /tmp/spark-events && chmod -R 777 /tmp/.ivy2 /tmp/spark-events
              /opt/spark/sbin/start-thriftserver.sh \
                --master spark://spark-master-svc:7077 \
                --hiveconf hive.metastore.uris=thrift://hive-metastore-svc:9083 \
                --conf spark.sql.catalogImplementation=hive \
                --conf spark.sql.warehouse.dir=s3a://${S3_BUCKET}/warehouse \
                --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
                --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
                --conf spark.sql.hive.metastore.version=4.0.1 \
                --conf spark.sql.hive.metastore.jars=maven \
                --conf spark.jars.ivy=/tmp/.ivy2 \
                --conf spark.driver.extraJavaOptions="-Duser.home=/tmp -Divy.home=/tmp/.ivy2" \
                --conf spark.hadoop.fs.s3a.endpoint=${S3_ENDPOINT} \
                --conf spark.hadoop.fs.s3a.path.style.access=true \
                --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
                --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
                --conf spark.driver.extraClassPath=/opt/spark/ext-jars/* \
                --conf spark.driver.bindAddress=0.0.0.0 \
                --conf spark.driver.host=spark-thrift-svc \
                --conf spark.driver.port=39000 \
                --conf spark.blockManager.port=39001 \
                --conf spark.executor.cores=2 \
                --conf spark.cores.max=4
              tail -f /opt/spark/logs/* &
              wait
          env:
            - name: HOME
              value: "/tmp"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: S3_ACCESS_KEY
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: S3_SECRET_KEY
            - name: S3_ENDPOINT
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: S3_ENDPOINT
            - name: S3_BUCKET
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: S3_BUCKET
          ports:
            - containerPort: 10000
            - containerPort: 4040
            - containerPort: 39000
            - containerPort: 39001
          volumeMounts:
            - name: spark-defaults
              mountPath: /opt/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: hadoop-core-site
              mountPath: /opt/spark/conf/core-site.xml
              subPath: core-site.xml
            - name: ext-jars
              mountPath: /opt/spark/ext-jars
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "2"
              memory: "4Gi"
      volumes:
        - name: spark-defaults
          configMap:
            name: spark-defaults
        - name: hadoop-core-site
          configMap:
            name: spark-hadoop-core-site
        - name: ext-jars
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: spark-thrift-svc
spec:
  type: ClusterIP
  selector:
    app: spark
    role: thrift
  ports:
    - name: thrift
      port: 10000
      targetPort: 10000
    - name: ui
      port: 4040
      targetPort: 4040
    - name: driver-rpc
      port: 39000
      targetPort: 39000
    - name: blockmanager
      port: 39001
      targetPort: 39001
