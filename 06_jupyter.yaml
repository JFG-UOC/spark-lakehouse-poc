apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jupyter-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: microk8s-hostpath
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupyter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupyter
  template:
    metadata:
      labels:
        app: jupyter
    spec:
      initContainers:
        - name: install-java
          image: curlimages/curl:8.11.1
          command:
            - sh
            - -c
            - |
              set -e
              echo "Creating /opt/java..."
              mkdir -p /opt/java

              echo "Downloading OpenJDK 11.0.27..."
              cd /tmp
              curl -fsSL -o openjdk.tar.gz \
                https://github.com/adoptium/temurin11-binaries/releases/download/jdk-11.0.27+6/OpenJDK11U-jdk_x64_linux_hotspot_11.0.27_6.tar.gz

              echo "Extracting OpenJDK..."
              tar -xzf openjdk.tar.gz
              rm -f openjdk.tar.gz

              echo "Moving OpenJDK to /opt/java..."
              mv jdk-11.0.27+6/* /opt/java/
              rm -rf "jdk-11.0.27+6"

              echo "Java installed at /opt/java"
          volumeMounts:
            - name: java-dist
              mountPath: /opt/java

        - name: setup-spark
          image: curlimages/curl:8.11.1
          command:
            - sh
            - -c
            - |
              set -e

              echo "Creating /opt/spark..."
              mkdir -p /opt/spark

              echo "Downloading Spark 3.5.6..."
              cd /tmp
              curl -fsSL -o spark-3.5.6-bin-hadoop3.tgz \
                https://archive.apache.org/dist/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz

              echo "Extracting Spark..."
              tar -xzf spark-3.5.6-bin-hadoop3.tgz -C /tmp
              rm -f spark-3.5.6-bin-hadoop3.tgz

              echo "Moving Spark to /opt/spark..."
              mv /tmp/spark-3.5.6-bin-hadoop3/* /opt/spark/
              rm -rf /tmp/spark-3.5.6-bin-hadoop3

              echo "Preparing ext-jars directory..."
              mkdir -p /opt/spark/ext-jars

              echo 'Downloading hadoop-aws...';
              curl -fsSL -o /opt/spark/ext-jars/hadoop-aws-3.3.4.jar \
                https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar;

              echo 'Downloading AWS SDK...';
              curl -fsSL -o /opt/spark/ext-jars/aws-java-sdk-bundle-1.12.262.jar \
                https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar;

              echo 'Downloading Delta Lake...';
              curl -fsSL -o /opt/spark/ext-jars/delta-spark_2.12-3.3.2.jar \
                https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.2/delta-spark_2.12-3.3.2.jar;
              curl -fsSL -o /opt/spark/ext-jars/delta-storage-3.3.2.jar \
                https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.2/delta-storage-3.3.2.jar;

              echo 'Downloading RAPIDS...';
              curl -fsSL -o /opt/spark/ext-jars/rapids-4-spark_2.12-25.10.0.jar \
                https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/25.10.0/rapids-4-spark_2.12-25.10.0.jar;
              curl -fsSL -o /opt/spark/ext-jars/cudf-25.10.0-cuda12.jar \
                https://repo1.maven.org/maven2/ai/rapids/cudf/25.10.0/cudf-25.10.0-cuda12.jar;

              echo "Spark ready in /opt/spark"
          volumeMounts:
            - name: spark-dist
              mountPath: /opt/spark

      containers:
        - name: jupyter
          image: jupyter/datascience-notebook:python-3.8
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8888

          command:
            - bash
            - -lc
          args:
            - |
              pip install --no-cache-dir pyspark==3.5.6
              start-notebook.sh

          env:
            - name: JUPYTER_TOKEN
              value: "poc"
            - name: JAVA_HOME
              value: "/opt/java"
            - name: SPARK_HOME
              value: "/opt/spark"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_USER
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: MINIO_ROOT_PASSWORD

            - name: JAVA_TOOL_OPTIONS
              value: >
                -Dspark.master=spark://spark-master-svc:7077
                -Dspark.driver.host=jupyter-svc
                -Dspark.driver.bindAddress=0.0.0.0
                -Dspark.driver.port=39000
                -Dspark.blockManager.port=39001
                -Dspark.executor.extraClassPath=/opt/spark/ext-jars/*
                -Dspark.driver.extraClassPath=/opt/spark/ext-jars/*
                -Dhadoop.fs.s3a.endpoint=http://minio-svc:9000
                -Dhadoop.fs.s3a.path.style.access=true
                -Dhadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
                -Dhadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.EnvironmentVariableCredentialsProvider

          volumeMounts:
            - name: java-dist
              mountPath: /opt/java
            - name: spark-dist
              mountPath: /opt/spark
            - name: hadoop-core-site
              mountPath: /opt/spark/conf/core-site.xml
              subPath: core-site.xml
            - name: spark-defaults
              mountPath: /opt/spark/conf/spark-defaults.conf
              subPath: spark-defaults.conf
            - name: jupyter-data
              mountPath: /home/jovyan/work

      volumes:
        - name: java-dist
          emptyDir: {}
        - name: spark-dist
          emptyDir: {}
        - name: hadoop-core-site
          configMap:
            name: spark-hadoop-core-site
        - name: spark-defaults
          configMap:
            name: spark-defaults
        - name: jupyter-data
          persistentVolumeClaim:
            claimName: jupyter-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: jupyter-svc
spec:
  type: ClusterIP
  ports:
    - name: notebook
      port: 8888
      targetPort: 8888
    - name: driver-rpc
      port: 39000
      targetPort: 39000
    - name: blockmanager
      port: 39001
      targetPort: 39001
  selector:
    app: jupyter
